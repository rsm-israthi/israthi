[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Isha",
    "section": "",
    "text": "Welcome to my website!"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Resume",
    "section": "",
    "text": "{{&lt; pdf files/Resume.pdf 100% 800 &gt;}}"
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of -0.85.\n\n\nHere is a plot:\n\nlibrary(ggplot2)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "projects/project1/index.html#sub-header",
    "href": "projects/project1/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:\n\nlibrary(ggplot2)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nIsha Rathi\n\n\nMay 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Cars\n\n\n\n\n\n\nYour Name\n\n\nMay 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Model\n\n\n\n\n\n\nIsha Rathi\n\n\nMay 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nIsha Rathi\n\n\nMay 24, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/project2/hw1_questions.html",
    "href": "projects/project2/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nTo expand on the experimental design: the letters were sent to individuals who had previously donated to the same nonprofit organization, ensuring that the recipients were already familiar with the cause. In the matching grant treatments, recipients were told that a generous donor would match their contribution at one of three possible rates: $1:$1, $2:$1, or $3:$1. This created a natural variation in the “price” of giving, where a $50 donation could yield $100, $150, or even $200 for the charity, depending on the match.\nThe researchers also varied the maximum size of the matching grant (e.g., $25,000, $50,000, $100,000, or left unspecified) to test whether the perceived urgency or credibility of the match offer influenced giving. Additionally, the suggested donation amount was customized for each recipient based on their past contributions and randomized to test how the framing of the “ask” amount affects behavior.\nThis experiment provided a rare opportunity to test economic theories about public goods and altruism in a real-world setting with actual financial stakes. The findings have since become a cornerstone in the field of behavioral economics and nonprofit fundraising.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project2/hw1_questions.html#introduction",
    "href": "projects/project2/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nTo expand on the experimental design: the letters were sent to individuals who had previously donated to the same nonprofit organization, ensuring that the recipients were already familiar with the cause. In the matching grant treatments, recipients were told that a generous donor would match their contribution at one of three possible rates: $1:$1, $2:$1, or $3:$1. This created a natural variation in the “price” of giving, where a $50 donation could yield $100, $150, or even $200 for the charity, depending on the match.\nThe researchers also varied the maximum size of the matching grant (e.g., $25,000, $50,000, $100,000, or left unspecified) to test whether the perceived urgency or credibility of the match offer influenced giving. Additionally, the suggested donation amount was customized for each recipient based on their past contributions and randomized to test how the framing of the “ask” amount affects behavior.\nThis experiment provided a rare opportunity to test economic theories about public goods and altruism in a real-world setting with actual financial stakes. The findings have since become a cornerstone in the field of behavioral economics and nonprofit fundraising.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project2/hw1_questions.html#data",
    "href": "projects/project2/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\n\nReading the Data\nWe use the dataset made available by the authors, which contains over 50,000 observations corresponding to individual donors who received various fundraising letter treatments.\n\nlibrary(haven)\ndf &lt;- read_dta(\"files/karlan_list_2007.dta\")\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nTo evaluate whether the random assignment of treatment was successful, we compare the distributions of a few pre-treatment variables between the treatment and control groups.\nWe begin by examining mrm2 — the number of months since last donation. If treatment was truly randomly assigned, we should not observe a statistically significant difference in this variable across groups. We test this using both a two-sample t-test and a linear regression.\n\n# Load required package\nlibrary(dplyr)\n\n# Clean the data\ndf_clean &lt;- df %&gt;% filter(!is.na(mrm2))\n\nTwo-sample t-test:\n\nlibrary(broom)\ntidy(t.test(mrm2 ~ treatment, data = df_clean))\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1  -0.0137      13.0      13.0    -0.120   0.905    33394.   -0.238     0.211\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nLinear regression:\n\nlibrary(broom)\n# Run regression\nmodel &lt;- lm(mrm2 ~ treatment, data = df_clean)\n# Format output\ntidy(model)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  13.0       0.0935   139.      0    \n2 treatment     0.0137    0.115      0.119   0.905\n\n\n\n\nAnalysis:\nAs expected, both the t-test and the regression yield the same estimate and same p-value. The coefficient on treatment from the regression indicates the difference in average months since last donation between the treatment and control groups.\nIf this difference is not statistically significant (p &gt; 0.05), it supports the idea that treatment was assigned randomly and not correlated with past donation behavior.\nThis logic is also why Table 1 is included in the original Karlan and List paper — to show that observable characteristics are balanced across groups. If they weren’t, we might worry that any difference in giving could be due to those differences rather than the treatment itself."
  },
  {
    "objectID": "projects/project2/hw1_questions.html#experimental-results",
    "href": "projects/project2/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nBelow is a barplot comparing the proportion of individuals who donated in the treatment and control groups.\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Prepare labeled summary\ndonation_rates &lt;- df %&gt;%\n  mutate(group = ifelse(treatment == 1, \"Treatment\", \"Control\")) %&gt;%\n  group_by(group) %&gt;%\n  summarise(prop_donated = mean(gave, na.rm = TRUE))\n\nggplot(donation_rates, aes(x = group, y = prop_donated, fill = group)) +\n  geom_col(width = 0.6) +\n  labs(\n    title = \"Proportion of Donors by Treatment Group\",\n    x = \"Group\",\n    y = \"Proportion Donated\"\n  ) +\n  scale_fill_manual(values = c(\"Control\" = \"lightsalmon\", \"Treatment\" = \"orange\")) +\n  theme_minimal() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(hjust = 0.5),\n    axis.text.x = element_text(face = \"bold\")\n  )"
  },
  {
    "objectID": "projects/project2/hw1_questions.html#simulation-experiment",
    "href": "projects/project2/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nWe simulate:\n\nControl group donations with a Bernoulli distribution: p = 0.018\nTreatment group donations with a Bernoulli distribution: p = 0.022\n100,000 draws each, and plot the running average of the differences.\n\n\nset.seed(123)  # for reproducibility\n\n# Simulate outcomes\ncontrol_sim &lt;- rbinom(100000, 1, 0.018)\ntreat_sim &lt;- rbinom(10000, 1, 0.022)\n\n# Difference in donations\ndiffs &lt;- treat_sim - control_sim[1:10000]\n\n# Cumulative average of the differences\ncum_avg &lt;- cumsum(diffs) / seq_along(diffs)\n# Plot\nlibrary(ggplot2)\ndf &lt;- data.frame(\n  Simulations = 1:10000,\n  CumulativeAverage = cum_avg\n)\n\nggplot(df, aes(x = Simulations)) +\n  geom_line(aes(y = CumulativeAverage, color = \"Cumulative Average of Differences\"), size = 0.7) +\n  geom_hline(aes(yintercept = 0.004, color = \"True Mean Difference (0.022 - 0.018)\", linetype = \"True Mean Difference (0.022 - 0.018)\"), size = 1) +\n  scale_color_manual(\n    name = \"\",\n    values = c(\n      \"Cumulative Average of Differences\" = \"#1f77b4\",\n      \"True Mean Difference (0.022 - 0.018)\" = \"red\"\n    )\n  ) +\n  scale_linetype_manual(\n    name = \"\",\n    values = c(\"True Mean Difference (0.022 - 0.018)\" = \"dashed\")\n  ) +\n  labs(\n    title = \"Law of Large Numbers: Convergence of Mean Difference\",\n    x = \"Number of Simulations\",\n    y = \"Cumulative Average Difference\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    legend.position = \"top\"\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nInterpretation The blue line shows the cumulative average of donation rate differences as we simulate more and more respondents. The red dashed line shows the true underlying difference in means (0.004).\nAs the number of draws increases, the cumulative average steadily converges to the true difference of 0.004. This is a direct visual confirmation of the Law of Large Numbers: with enough observations, our sample estimate gets arbitrarily close to the population truth.\nThis simulation helps illustrate why large samples are powerful — they reduce noise and allow the average to stabilize around the expected value.\n\n\nCentral Limit Theorem\nThe Central Limit Theorem (CLT) tells us that the sampling distribution of the sample mean will approach a normal distribution as the sample size increases — even if the underlying data is binary.\nTo demonstrate this, we simulate 1,000 experiments for different sample sizes and observe how the distribution of average treatment effects changes.\n\nset.seed(42)\n\nsample_sizes &lt;- c(50, 200, 500, 1000)\ntrue_diff &lt;- 0.022 - 0.018\n\npar(mfrow = c(2, 2))  # Set up 2x2 grid of plots\n\nfor (n in sample_sizes) {\n  diffs &lt;- replicate(1000, {\n    control &lt;- rbinom(n, 1, 0.018)\n    treatment &lt;- rbinom(n, 1, 0.022)\n    mean(treatment) - mean(control)\n  })\n  \n  hist(diffs, breaks = 30, col = \"lightblue\", border = \"black\",\n       main = paste(\"Sample Size:\", n),\n       xlab = \"Average (Treatment - Control)\")\n  abline(v = 0, col = \"black\", lty = 2)\n  abline(v = true_diff, col = \"red\", lty = 2)\n  legend(\"topright\", legend = sprintf(\"True Diff = %.4f\", true_diff),\n         lty = 2, col = \"red\", bty = \"n\")\n}\n\n\n\n\n\n\n\n\nInterpretation Each histogram shows the sampling distribution of the difference in means (treatment - control) for 1,000 simulated experiments at a given sample size.\n\nAt n = 50, the distribution is wide and bumpy, and 0 is well within the bulk, reflecting high variability.\nAs n increases to 200, 500, and 1000, the distribution becomes narrower and more bell-shaped.\nBy n = 1000, the distribution is quite tight and centered around the true difference (~0.004), and 0 lies clearly in the tail.\n\nThis confirms the CLT: with larger sample sizes, the sampling distribution of the mean difference becomes approximately normal, and extreme values (like 0) become less probable if the true effect is nonzero."
  },
  {
    "objectID": "projects/project2/hw1_questions.html#reading-the-data",
    "href": "projects/project2/hw1_questions.html#reading-the-data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Reading the Data",
    "text": "Reading the Data\nWe use the dataset made available by the authors, which contains over 50,000 observations corresponding to individual donors who received various fundraising letter treatments.\n\nlibrary(haven)\ndf &lt;- read_dta(\"files/karlan_list_2007.dta\")\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)."
  },
  {
    "objectID": "projects/project2/hw1_questions.html#response-rate-treatment-vs-control",
    "href": "projects/project2/hw1_questions.html#response-rate-treatment-vs-control",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Response Rate: Treatment vs Control",
    "text": "Response Rate: Treatment vs Control\nTo understand whether matched donation offers influenced behavior, we test whether individuals in the treatment group were more likely to donate at all (gave = 1) compared to the control group. We use both a two-sample t-test and a bivariate linear regression.\n\nT-Test\n\n# T-test comparing donation rates between treatment and control\ntidy(t.test(gave ~ treatment, data = df))\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 -0.00418    0.0179    0.0220     -3.21 0.00133    36577. -0.00673  -0.00163\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\n\n\nRegression\n\n# Regression: same comparison as t-test\nmodel_gave &lt;- lm(gave ~ treatment, data = df)\ntidy(model_gave)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  0.0179    0.00110     16.2  4.78e-59\n2 treatment    0.00418   0.00135      3.10 1.93e- 3\n\n\nInterpretation\nBoth the t-test and the regression test whether the proportion of people who donated differs between the treatment and control groups. - The t-test gives us the raw comparison of means. - The regression gives us a coefficient on treatment, which reflects the increase in probability of donating due to receiving the match offer.\nIn this case, we find a small but statistically significant increase in donation response rate among those who received the matching offer. This result aligns with Table 2A, Panel A, which shows the response rate increased from 1.8% (control) to 2.2% (treatment).\nThis suggests that even subtle changes in messaging — such as adding a matching donation offer — can meaningfully influence behavior. It highlights that people are responsive to perceived leverage in giving: when told their gift will be matched, they are slightly more likely to give, even though the gift itself costs them the same amount.\nThis finding supports a key behavioral insight: framing matters. When giving feels more “impactful,” people are more inclined to act, even if the actual mechanics of the donation haven’t changed."
  },
  {
    "objectID": "projects/project2/hw1_questions.html#probit-regression-donation-on-treatment-assignment",
    "href": "projects/project2/hw1_questions.html#probit-regression-donation-on-treatment-assignment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Probit Regression: Donation on Treatment Assignment",
    "text": "Probit Regression: Donation on Treatment Assignment\nWe now estimate a probit regression model to explore whether receiving a matching donation offer increases the likelihood of making any charitable donation. The binary outcome variable is gave, and the sole explanatory variable is treatment.\nThis reproduces Column 1 of Table 3 from Karlan & List (2007), which shows the marginal effect of being in the treatment group.\n\nlibrary(margins)\n\n# Reuse the probit model\nprobit_model &lt;- glm(gave ~ treatment, data = df, family = binomial(link = \"probit\"))\n\n\n\n  0.004313\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\nTo test whether larger match ratios (2:1 or 3:1) significantly affect the likelihood of donating compared to the 1:1 match ratio, I conduct a series of t-tests on the binary gave outcome within the treatment group.\n\n# Filter only people in the treatment group\ntreat_only &lt;- df %&gt;% filter(treatment == 1)\n\n# T-test: 1:1 vs 2:1\nt1_vs_2 &lt;- t.test(gave ~ ratio2, data = treat_only %&gt;% filter(ratio2 + ratio3 == 0 | ratio2 == 1))\n\n# T-test: 1:1 vs 3:1\nt1_vs_3 &lt;- t.test(gave ~ ratio3, data = treat_only %&gt;% filter(ratio2 + ratio3 == 0 | ratio3 == 1))\n\n# Show results\ntidy(t1_vs_2)\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 -0.00188    0.0207    0.0226    -0.965   0.335    22225. -0.00571   0.00194\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\ntidy(t1_vs_3)\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 -0.00198    0.0207    0.0227     -1.02   0.310    22215. -0.00582   0.00185\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nInterpretation These t-tests compare the proportion of people who donated at:\n1:1 match (baseline group where ratio2 = 0 and ratio3 = 0) 2:1 match (ratio2 = 1) 3:1 match (ratio3 = 1)\nIf the p-values are high (e.g. &gt; 0.05), we fail to reject the null hypothesis — meaning there’s no statistically significant difference in donation likelihood between the higher match and the baseline 1:1.\nThis would support the authors’ conclusion on page 8 that:\n“… larger match ratios had no additional impact.”\nInsight: While offering a match boosts donations overall, increasing the size of the match doesn’t lead to a further increase. People respond to the presence of a match — not its generosity.\n\n\nRegression: Do Higher Match Ratios Increase Donations?\nTo more formally assess whether the match ratio impacts the likelihood of donating, we run a regression of gave on three binary indicators for match ratio.\nWe create dummy variables for each match level: - ratio1: 1:1 match - ratio2: 2:1 match - ratio3: 3:1 match\nSince all treatment group members received one of these three match offers, these indicators are mutually exclusive and collectively exhaustive.\n\n# Create dummy variable for 1:1 match\ndf &lt;- df %&gt;%\n  mutate(ratio1 = ifelse(ratio2 == 0 & ratio3 == 0, 1, 0))\n\n# Filter treatment group only\ntreat_df &lt;- df %&gt;% filter(treatment == 1)\n\n# Regression: gave ~ ratio1 + ratio2 + ratio3\nmodel_ratios &lt;- lm(gave ~ ratio1 + ratio2 + ratio3, data = treat_df)\n\n# Tidy the result\ntidy(model_ratios)\n\n# A tibble: 4 × 5\n  term         estimate std.error statistic   p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  0.0227     0.00139   16.3     9.44e-60\n2 ratio1      -0.00198    0.00197   -1.01    3.13e- 1\n3 ratio2      -0.000100   0.00197   -0.0508  9.59e- 1\n4 ratio3      NA         NA         NA      NA       \n\n\nInterpretation The coefficients on ratio2 and ratio3 represent the difference in probability of donating relative to the omitted group (1:1 match). The baseline category (ratio1 = 1) is captured by the intercept. If the coefficients on ratio2 and ratio3 are not statistically significant (p &gt; 0.05), this provides further evidence that larger match ratios do not meaningfully boost response rates.\nThis finding is consistent with the earlier t-tests and the authors’ claim that “larger match ratios had no additional impact.”\n\n\nComparing Response Rates Across Match Ratios\nWe compare whether increasing the size of the match — from 1:1 to 2:1, and from 2:1 to 3:1 — leads to a higher response rate (i.e., probability of donation).\n\n\n1. Response Rate Differences (Direct from Data)\n\n# Directly calculate response rate by match ratio\nmatch_rates &lt;- df %&gt;%\n  filter(treatment == 1) %&gt;%\n  group_by(ratio) %&gt;%\n  summarise(response_rate = mean(gave, na.rm = TRUE))\n\nmatch_rates\n\n# A tibble: 3 × 2\n  ratio     response_rate\n  &lt;dbl+lbl&gt;         &lt;dbl&gt;\n1 1                0.0207\n2 2                0.0226\n3 3                0.0227\n\n\n\n# Calculate differences manually\ndiff_2_1_vs_1_1 &lt;- match_rates$response_rate[match_rates$ratio == 2] -\n                   match_rates$response_rate[match_rates$ratio == 1]\n\ndiff_3_1_vs_2_1 &lt;- match_rates$response_rate[match_rates$ratio == 3] -\n                   match_rates$response_rate[match_rates$ratio == 2]\n\ndiff_2_1_vs_1_1\n\n[1] 0.001884251\n\ndiff_3_1_vs_2_1\n\n[1] 0.000100024\n\n\n\n\nCoefficient difference from previous regression model\n\ncoef_diff_2_1_vs_1_1 &lt;- coef(model_ratios)[\"ratio2\"] - coef(model_ratios)[\"ratio1\"]\ncoef_diff_3_1_vs_2_1 &lt;- coef(model_ratios)[\"ratio3\"] - coef(model_ratios)[\"ratio2\"]\n\ncoef_diff_2_1_vs_1_1\n\n     ratio2 \n0.001884251 \n\ncoef_diff_3_1_vs_2_1\n\nratio3 \n    NA \n\n\nInterpretation\nIn both the direct data comparison and model-based comparison, you’re likely to see:\nA small or negligible increase from 1:1 to 2:1 No further gain or even a decline from 2:1 to 3:1\nThis suggests that people are responsive to the presence of a match, but not particularly sensitive to how generous it is — a powerful insight for fundraisers: it’s the existence of a match, not its size, that matters.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n# T-test: unconditional amount given\ntidy(t.test(amount ~ treatment, data = df))\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1   -0.154     0.813     0.967     -1.92  0.0551    36216.   -0.311   0.00334\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nKey takeaway: Offering a matching donation increases the average amount raised per person, even when accounting for all individuals (including those who gave nothing). This reinforces the behavioral insight that the presence of a match not only increases participation but also increases the total dollars raised.\n\n\nSize of Charitable Contribution (Conditional on Donating)\nNext, we restrict the data to only those who actually donated (gave == 1) and repeat the analysis. This helps us understand whether the treatment affected how much people gave, among those who chose to give.\n\n# Filter to donors only\ndf_donors &lt;- df %&gt;% filter(gave == 1)\n\n# T-test: donation amount by treatment (among donors)\ntidy(t.test(amount ~ treatment, data = df_donors))\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     1.67      45.5      43.9     0.585   0.559      557.    -3.94      7.27\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n# Linear regression: donation amount on treatment\nlm_donors &lt;- lm(amount ~ treatment, data = df_donors)\ntidy(lm_donors)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    45.5       2.42    18.8   5.47e-68\n2 treatment      -1.67      2.87    -0.581 5.61e- 1\n\n\nInterpretation The regression coefficient on treatment now tells us whether people who gave donated more if they were offered a match. According to Table 2A of the paper, conditional donation amounts are about the same across groups — around $45. So you’ll likely find the treatment coefficient is small and statistically insignificant.\nConclusion: The treatment increased the number of people who gave, and therefore increased total revenue, but it did not significantly change the donation size among those who were already going to donate.\nCausal Interpretation The treatment effect on donation size (conditional on giving) does not have a clean causal interpretation. Because we’re conditioning on a post-treatment behavior (gave == 1), which introduces selection bias. The people who gave in the control group may be different (in motivation, wealth, etc.) than those who gave in the treatment group. So the result is descriptive, not causal — it’s still interesting, but we can’t claim the match offer caused people to give more per person among givers.\n\n\nDistribution of Donation Amounts Among Donors\nTo better understand the distribution of how much people gave, we plot histograms of donation amounts for both treatment and control groups — limited to only those who donated. Each plot includes a red vertical line marking the mean donation for that group.\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Filter to only donors\ndf_donors &lt;- df %&gt;% filter(gave == 1)\n\n# Add group label\ndf_donors &lt;- df_donors %&gt;%\n  mutate(group = ifelse(treatment == 1, \"Treatment\", \"Control\"))\n\n# Compute group means for vlines\nmean_donations &lt;- df_donors %&gt;%\n  group_by(group) %&gt;%\n  summarise(avg = mean(amount), .groups = \"drop\")\n\n# Merge means back into original data\ndf_donors &lt;- df_donors %&gt;%\n  left_join(mean_donations, by = \"group\")\n\n# Plot using facet_wrap\nggplot(df_donors, aes(x = amount)) +\n  geom_histogram(binwidth = 5, fill = \"lightyellow\", color = \"black\") +\n  geom_vline(aes(xintercept = avg), color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  facet_wrap(~ group, ncol = 2) +\n  labs(\n    title = \"Distribution of Donation Amounts Among Donors\",\n    x = \"Donation Amount\",\n    y = \"Number of Donors\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n    strip.text = element_text(size = 12, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\nInterpretation The histograms above show the distribution of donation amounts among individuals who chose to donate, separated by treatment group.\nThe red dashed line in each panel marks the mean donation for that group.\nVisually, both distributions are heavily right-skewed, with most donations clustered below $100 but a few large contributions stretching the range. The mean donation is very similar between the treatment and control groups, consistent with earlier statistical results (like the conditional t-test and regression).\nThis supports the conclusion that while the matching offer increased the likelihood of giving, it did not significantly affect how much donors gave once they decided to give.\nIn other words: the treatment affected the extensive margin (whether to give), not the intensive margin (how much to give)."
  },
  {
    "objectID": "projects/project2/hw1_questions.html#regression-do-higher-match-ratios-increase-donations",
    "href": "projects/project2/hw1_questions.html#regression-do-higher-match-ratios-increase-donations",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Regression: Do Higher Match Ratios Increase Donations?",
    "text": "Regression: Do Higher Match Ratios Increase Donations?\nTo more formally assess whether the match ratio impacts the likelihood of donating, we run a regression of gave on three binary indicators for match ratio.\nWe create dummy variables for each match level: - ratio1: 1:1 match - ratio2: 2:1 match - ratio3: 3:1 match\nSince all treatment group members received one of these three match offers, these indicators are mutually exclusive and collectively exhaustive.\n\n# Create dummy variable for 1:1 match\ndf &lt;- df %&gt;%\n  mutate(ratio1 = ifelse(ratio2 == 0 & ratio3 == 0, 1, 0))\n\n# Filter treatment group only\ntreat_df &lt;- df %&gt;% filter(treatment == 1)\n\n# Regression: gave ~ ratio1 + ratio2 + ratio3\nmodel_ratios &lt;- lm(gave ~ ratio1 + ratio2 + ratio3, data = treat_df)\n\n# Tidy the result\ntidy(model_ratios)\n\n# A tibble: 4 × 5\n  term         estimate std.error statistic   p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  0.0227     0.00139   16.3     9.44e-60\n2 ratio1      -0.00198    0.00197   -1.01    3.13e- 1\n3 ratio2      -0.000100   0.00197   -0.0508  9.59e- 1\n4 ratio3      NA         NA         NA      NA       \n\n\nInterpretation The coefficients on ratio2 and ratio3 represent the difference in probability of donating relative to the omitted group (1:1 match).\nThe baseline category (ratio1 = 1) is captured by the intercept.\nIf the coefficients on ratio2 and ratio3 are not statistically significant (p &gt; 0.05), this provides further evidence that larger match ratios do not meaningfully boost response rates.\nThis finding is consistent with the earlier t-tests and the authors’ claim that “larger match ratios had no additional impact.”\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot."
  },
  {
    "objectID": "projects/project3/hw2_questions.html",
    "href": "projects/project3/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nlibrary(readr)\nlibrary(dplyr)\n\n\nlibrary(ggplot2)\n\n# Read in the dataset\ndf &lt;- read_csv(\"files/blueprinty.csv\", show_col_types = FALSE)\n\nglimpse(df)\n\nRows: 1,500\nColumns: 4\n$ patents    &lt;dbl&gt; 0, 3, 4, 3, 3, 6, 5, 5, 6, 4, 2, 3, 7, 4, 5, 4, 2, 2, 2, 5,…\n$ region     &lt;chr&gt; \"Midwest\", \"Southwest\", \"Northwest\", \"Northeast\", \"Southwes…\n$ age        &lt;dbl&gt; 32.5, 37.5, 27.0, 24.5, 37.0, 29.5, 27.0, 20.5, 25.0, 29.5,…\n$ iscustomer &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,…\n\n\n\n# Convert iscustomer to factor for labeling\ndf &lt;- df %&gt;% mutate(customer_status = factor(iscustomer, labels = c(\"Non-Customer\", \"Customer\")))\n\n# Mean patents by customer status\ndf %&gt;%\n  group_by(customer_status) %&gt;%\n  summarise(mean_patents = mean(patents), .groups = \"drop\")\n\n# A tibble: 2 × 2\n  customer_status mean_patents\n  &lt;fct&gt;                  &lt;dbl&gt;\n1 Non-Customer            3.47\n2 Customer                4.13\n\n# Histogram\nggplot(df, aes(x = patents, fill = customer_status)) +\n  geom_histogram(alpha = 0.6, position = \"identity\", bins = 30) +\n  facet_wrap(~customer_status) +\n  labs(title = \"Patent Count Distribution by Customer Status\",\n       x = \"Number of Patents\",\n       y = \"Number of Firms\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\n\nThe histogram comparing patent counts between Blueprinty customers and non-customers reveals a clear difference in distributions. Non-customer firms tend to cluster around 2 to 4 patents, with relatively few exceeding 10. In contrast, customer firms not only peak slightly higher but also display a broader spread, with a noticeable number achieving 10 or more patents. This suggests that Blueprinty users may be more productive in securing patents.\nThis visual pattern is supported by the mean values: firms using Blueprinty software have an average of 4.13 patents, compared to 3.47 patents for non-customers. While the difference is modest (approximately 0.66 patents), it aligns with the distributional differences seen in the histogram and suggests a positive association between software usage and patent output.\nHowever, it is important to note that this analysis shows a correlation, not causation. Other factors such as firm age, size, or location may also influence patent outcomes—these will be explored in further analysis.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\n# REGION: Count of firms by region and customer status\nregion_summary &lt;- df %&gt;%\n  count(region, customer_status)\n\n# Plot: Region-wise distribution\nggplot(region_summary, aes(x = region, y = n, fill = customer_status)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Number of Firms by Region and Customer Status\",\n       x = \"Region\", y = \"Number of Firms\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# AGE: Boxplot of firm age by customer status\nggplot(df, aes(x = customer_status, y = age, fill = customer_status)) +\n  geom_boxplot() +\n  labs(title = \"Firm Age by Customer Status\",\n       x = \"Customer Status\", y = \"Firm Age (years)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\n\nBlueprinty customers are disproportionately concentrated in the Northeast, while non-customers dominate other regions like the Midwest and South. This suggests regional differences in software adoption.\n\n\nIn terms of age, customer firms are slightly younger on average, though the difference is modest. While the difference is not extreme, it may indicate that newer firms are more likely to adopt Blueprinty’s software, possibly due to greater tech adoption or strategic orientation.Both groups show similar median ages around 25 years.\n\nThese patterns suggest that region and firm age may influence patent outcomes and should be considered in further analysis.\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\\[\nY_i \\sim \\text{Poisson}(\\lambda)\n\\]\nThe probability mass function for a Poisson-distributed variable is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nWe now write the likelihood function for a sample of ( n ) independent observations ( Y_1, Y_2, , Y_n ):\n\\[\nL(\\lambda \\mid Y_1, \\dots, Y_n) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nThis simplifies to:\n\\[\nL(\\lambda) = \\frac{e^{-n\\lambda} \\lambda^{\\sum Y_i}}{\\prod_{i=1}^{n} Y_i!}\n\\]\nTaking the natural logarithm gives the log-likelihood function:\n\\[\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda - \\sum_{i=1}^{n} \\log Y_i!\n\\]\nThis function will form the basis for estimating ( ) via Maximum Likelihood Estimation (MLE).\nDefine Poisson log-likelihood function\n\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  if (lambda &lt;= 0) return(-Inf)  # log-likelihood is undefined for non-positive lambda\n  \n  n &lt;- length(Y)\n  log_lik &lt;- -n * lambda + sum(Y) * log(lambda) - sum(lfactorial(Y))\n  return(log_lik)\n}\n\nPlotting the Log-Likelihood for Varying Lambda\n\n# Vector of observed patent counts\nY &lt;- df$patents\n\n# Define a range of lambda values\nlambda_vals &lt;- seq(0.1, 10, by = 0.1)\n\n# Compute log-likelihood for each lambda\nloglik_vals &lt;- sapply(lambda_vals, poisson_loglikelihood, Y = Y)\n\n# Plot\nplot(lambda_vals, loglik_vals, type = \"l\", lwd = 2,\n     xlab = expression(lambda),\n     ylab = \"Log-Likelihood\",\n     main = \"Poisson Log-Likelihood Function\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\n\nThe plot shows the log-likelihood of the Poisson model across a range of λ values. The curve peaks at the maximum likelihood estimate (MLE), which corresponds to the λ that best explains the observed number of patents in the dataset.\n\n\n\nDeriving the MLE for λ in the Poisson Model\nWe begin with the log-likelihood function for a Poisson-distributed variable ( Y_1, , Y_n () ):\n\\[\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda - \\sum_{i=1}^{n} \\log Y_i!\n\\]\nSince the last term does not depend on ( ), we focus on the first two terms when maximizing:\n\\[\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum Y_i \\right) \\log \\lambda\n\\]\nTaking the derivative with respect to ( ):\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = -n + \\frac{\\sum Y_i}{\\lambda}\n\\]\nSet the derivative equal to zero to find the maximum:\n\\[\n-n + \\frac{\\sum Y_i}{\\lambda} = 0\n\\]\nSolving for ( ):\n\\[\n\\lambda_{\\text{MLE}} = \\frac{1}{n} \\sum Y_i = \\bar{Y}\n\\]\n\nThis result aligns with intuition: the mean of a Poisson distribution is ( ), so the sample mean is the natural estimator.\n\nFinding the MLE Using optim()\n\n# Negative log-likelihood (since optim minimizes)\nneg_loglik &lt;- function(lambda, Y) {\n  return(-poisson_loglikelihood(lambda, Y))\n}\n\n# Use optim() to find lambda that minimizes the negative log-likelihood\nmle_result &lt;- optim(par = 1, fn = neg_loglik, Y = df$patents, method = \"Brent\", lower = 0.01, upper = 10)\n\n# Print MLE estimate\nlambda_mle &lt;- mle_result$par\nlambda_mle\n\n[1] 3.684667\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nDefining the Poisson Regression Log-Likelihood Function\n\npoisson_regression_loglikelihood &lt;- function(beta, Y, X) {\n  # Linear predictor: X %*% beta gives a column vector\n  eta &lt;- X %*% beta\n  \n  # Inverse link function (log link): lambda = exp(X * beta)\n  lambda &lt;- exp(eta)\n  \n  # Log-likelihood function\n  log_lik &lt;- sum(-lambda + Y * log(lambda) - lfactorial(Y))\n  \n  return(log_lik)\n}\n\nThis function accepts: - beta: a vector of regression coefficients\n- Y: a vector of observed patent counts\n- X: a covariate matrix including firm-level predictors (e.g., age, region dummies, customer status)\nEstimating Poisson Regression with Covariates Using optim()\nWe now construct the design matrix ( X ), find the MLE of the coefficient vector ( ), and calculate standard errors using the inverse of the Hessian matrix.\nStep 1: Construct the Covariate Matrix\n\n# Create region dummies (drop one to avoid multicollinearity)\ndf &lt;- df %&gt;%\n  mutate(region = factor(region)) %&gt;%\n  mutate(age_sq = age^2)\n\nX &lt;- model.matrix(~ age + age_sq + region + iscustomer, data = df)\n\n# Outcome variable\nY &lt;- df$patents\n\n\nStep 2: Define the Negative Log-Likelihood for optim()\n\nneg_loglik_reg &lt;- function(beta, Y, X) {\n  eta &lt;- X %*% beta\n  lambda &lt;- exp(eta)\n  -sum(-lambda + Y * log(lambda) - lfactorial(Y))  # Negative log-likelihood\n}\n\n\nStep 3: Estimate MLE and Compute Hessian\n\n# Initial guess: zero vector\ninit_beta &lt;- rep(0, ncol(X))\n\n# Optimize\nfit &lt;- optim(par = init_beta,\n             fn = neg_loglik_reg,\n             Y = Y, X = X,\n             method = \"BFGS\", hessian = TRUE)\n\n# Extract estimates and variance-covariance matrix\nbeta_hat &lt;- fit$par\nhessian_mat &lt;- fit$hessian\nvcov_mat &lt;- solve(hessian_mat)  # Invert Hessian to get variance-covariance\nse_hat &lt;- sqrt(diag(vcov_mat))  # Standard errors\n\n\nStep 4: Present Results in a Table\n\n\n\nPoisson Regression Coefficients and Standard Errors\n\n\nTerm\nEstimate\nStd_Error\n\n\n\n\n(Intercept)\n-0.1257\n0.1122\n\n\nage\n0.1158\n0.0064\n\n\nage_sq\n-0.0022\n0.0001\n\n\nregionNortheast\n-0.0246\n0.0434\n\n\nregionNorthwest\n-0.0348\n0.0529\n\n\nregionSouth\n-0.0054\n0.0524\n\n\nregionSouthwest\n-0.0378\n0.0472\n\n\niscustomer\n0.0607\n0.0321\n\n\n\n\n\n\nThe table shows the estimated effect of each covariate on the log of the expected number of patents. The standard errors are derived from the inverse Hessian, assuming the log-likelihood is approximately quadratic near the maximum.\n\nChecking Results with glm()\nTo validate our MLE results, we fit the same Poisson regression model using R’s glm() function with the family = poisson option.\n\n# Fit Poisson regression using glm()\nglm_fit &lt;- glm(patents ~ age + I(age^2) + region + iscustomer,\n               data = df, family = poisson())\n\n# Summary of model\nsummary(glm_fit)\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = poisson(), data = df)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.508920   0.183179  -2.778  0.00546 ** \nage              0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)        -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast  0.029170   0.043625   0.669  0.50372    \nregionNorthwest -0.017574   0.053781  -0.327  0.74383    \nregionSouth      0.056561   0.052662   1.074  0.28281    \nregionSouthwest  0.050576   0.047198   1.072  0.28391    \niscustomer       0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nThe coefficient and standard error estimates obtained from glm() match closely with our custom implementation using optim(). This validates that our likelihood function and MLE approach are working as expected.\n\n\n\n\n\n\n\nInterpretation\n\n\n\nThe Poisson regression model estimates how various firm characteristics influence the expected number of patents awarded over the past 5 years. Key findings include:\n\nAge has a positive effect on patent output: older firms tend to secure more patents. However, the negative and statistically significant coefficient on age squared suggests diminishing returns — patent productivity increases with age, but at a decreasing rate.\nRegion effects are relatively small and statistically insignificant, indicating that geographic location (after controlling for other variables) does not strongly affect patent counts.\nMost importantly, the coefficient on iscustomer is 0.2076, which is statistically significant at the 0.001 level. Interpreted on the original scale:\n\\[\n\\exp(0.2076) - 1 \\approx 23\\%\n\\]\nThis means that, holding all else constant, firms that use Blueprinty software have an estimated 23% higher expected patent count than non-users.\n\n\nThese results support the marketing team’s claim: Blueprinty customers tend to secure more patents, even after adjusting for age and region. However, this is still a correlational model, and other unmeasured factors may contribute to this difference."
  },
  {
    "objectID": "projects/project3/hw2_questions.html#blueprinty-case-study",
    "href": "projects/project3/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nlibrary(readr)\nlibrary(dplyr)\n\n\nlibrary(ggplot2)\n\n# Read in the dataset\ndf &lt;- read_csv(\"files/blueprinty.csv\", show_col_types = FALSE)\n\nglimpse(df)\n\nRows: 1,500\nColumns: 4\n$ patents    &lt;dbl&gt; 0, 3, 4, 3, 3, 6, 5, 5, 6, 4, 2, 3, 7, 4, 5, 4, 2, 2, 2, 5,…\n$ region     &lt;chr&gt; \"Midwest\", \"Southwest\", \"Northwest\", \"Northeast\", \"Southwes…\n$ age        &lt;dbl&gt; 32.5, 37.5, 27.0, 24.5, 37.0, 29.5, 27.0, 20.5, 25.0, 29.5,…\n$ iscustomer &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,…\n\n\n\n# Convert iscustomer to factor for labeling\ndf &lt;- df %&gt;% mutate(customer_status = factor(iscustomer, labels = c(\"Non-Customer\", \"Customer\")))\n\n# Mean patents by customer status\ndf %&gt;%\n  group_by(customer_status) %&gt;%\n  summarise(mean_patents = mean(patents), .groups = \"drop\")\n\n# A tibble: 2 × 2\n  customer_status mean_patents\n  &lt;fct&gt;                  &lt;dbl&gt;\n1 Non-Customer            3.47\n2 Customer                4.13\n\n# Histogram\nggplot(df, aes(x = patents, fill = customer_status)) +\n  geom_histogram(alpha = 0.6, position = \"identity\", bins = 30) +\n  facet_wrap(~customer_status) +\n  labs(title = \"Patent Count Distribution by Customer Status\",\n       x = \"Number of Patents\",\n       y = \"Number of Firms\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\n\nThe histogram comparing patent counts between Blueprinty customers and non-customers reveals a clear difference in distributions. Non-customer firms tend to cluster around 2 to 4 patents, with relatively few exceeding 10. In contrast, customer firms not only peak slightly higher but also display a broader spread, with a noticeable number achieving 10 or more patents. This suggests that Blueprinty users may be more productive in securing patents.\nThis visual pattern is supported by the mean values: firms using Blueprinty software have an average of 4.13 patents, compared to 3.47 patents for non-customers. While the difference is modest (approximately 0.66 patents), it aligns with the distributional differences seen in the histogram and suggests a positive association between software usage and patent output.\nHowever, it is important to note that this analysis shows a correlation, not causation. Other factors such as firm age, size, or location may also influence patent outcomes—these will be explored in further analysis.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\n# REGION: Count of firms by region and customer status\nregion_summary &lt;- df %&gt;%\n  count(region, customer_status)\n\n# Plot: Region-wise distribution\nggplot(region_summary, aes(x = region, y = n, fill = customer_status)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Number of Firms by Region and Customer Status\",\n       x = \"Region\", y = \"Number of Firms\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# AGE: Boxplot of firm age by customer status\nggplot(df, aes(x = customer_status, y = age, fill = customer_status)) +\n  geom_boxplot() +\n  labs(title = \"Firm Age by Customer Status\",\n       x = \"Customer Status\", y = \"Firm Age (years)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\n\nBlueprinty customers are disproportionately concentrated in the Northeast, while non-customers dominate other regions like the Midwest and South. This suggests regional differences in software adoption.\n\n\nIn terms of age, customer firms are slightly younger on average, though the difference is modest. While the difference is not extreme, it may indicate that newer firms are more likely to adopt Blueprinty’s software, possibly due to greater tech adoption or strategic orientation.Both groups show similar median ages around 25 years.\n\nThese patterns suggest that region and firm age may influence patent outcomes and should be considered in further analysis.\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\\[\nY_i \\sim \\text{Poisson}(\\lambda)\n\\]\nThe probability mass function for a Poisson-distributed variable is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nWe now write the likelihood function for a sample of ( n ) independent observations ( Y_1, Y_2, , Y_n ):\n\\[\nL(\\lambda \\mid Y_1, \\dots, Y_n) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nThis simplifies to:\n\\[\nL(\\lambda) = \\frac{e^{-n\\lambda} \\lambda^{\\sum Y_i}}{\\prod_{i=1}^{n} Y_i!}\n\\]\nTaking the natural logarithm gives the log-likelihood function:\n\\[\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda - \\sum_{i=1}^{n} \\log Y_i!\n\\]\nThis function will form the basis for estimating ( ) via Maximum Likelihood Estimation (MLE).\nDefine Poisson log-likelihood function\n\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  if (lambda &lt;= 0) return(-Inf)  # log-likelihood is undefined for non-positive lambda\n  \n  n &lt;- length(Y)\n  log_lik &lt;- -n * lambda + sum(Y) * log(lambda) - sum(lfactorial(Y))\n  return(log_lik)\n}\n\nPlotting the Log-Likelihood for Varying Lambda\n\n# Vector of observed patent counts\nY &lt;- df$patents\n\n# Define a range of lambda values\nlambda_vals &lt;- seq(0.1, 10, by = 0.1)\n\n# Compute log-likelihood for each lambda\nloglik_vals &lt;- sapply(lambda_vals, poisson_loglikelihood, Y = Y)\n\n# Plot\nplot(lambda_vals, loglik_vals, type = \"l\", lwd = 2,\n     xlab = expression(lambda),\n     ylab = \"Log-Likelihood\",\n     main = \"Poisson Log-Likelihood Function\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\n\nThe plot shows the log-likelihood of the Poisson model across a range of λ values. The curve peaks at the maximum likelihood estimate (MLE), which corresponds to the λ that best explains the observed number of patents in the dataset.\n\n\n\nDeriving the MLE for λ in the Poisson Model\nWe begin with the log-likelihood function for a Poisson-distributed variable ( Y_1, , Y_n () ):\n\\[\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda - \\sum_{i=1}^{n} \\log Y_i!\n\\]\nSince the last term does not depend on ( ), we focus on the first two terms when maximizing:\n\\[\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum Y_i \\right) \\log \\lambda\n\\]\nTaking the derivative with respect to ( ):\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = -n + \\frac{\\sum Y_i}{\\lambda}\n\\]\nSet the derivative equal to zero to find the maximum:\n\\[\n-n + \\frac{\\sum Y_i}{\\lambda} = 0\n\\]\nSolving for ( ):\n\\[\n\\lambda_{\\text{MLE}} = \\frac{1}{n} \\sum Y_i = \\bar{Y}\n\\]\n\nThis result aligns with intuition: the mean of a Poisson distribution is ( ), so the sample mean is the natural estimator.\n\nFinding the MLE Using optim()\n\n# Negative log-likelihood (since optim minimizes)\nneg_loglik &lt;- function(lambda, Y) {\n  return(-poisson_loglikelihood(lambda, Y))\n}\n\n# Use optim() to find lambda that minimizes the negative log-likelihood\nmle_result &lt;- optim(par = 1, fn = neg_loglik, Y = df$patents, method = \"Brent\", lower = 0.01, upper = 10)\n\n# Print MLE estimate\nlambda_mle &lt;- mle_result$par\nlambda_mle\n\n[1] 3.684667\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nDefining the Poisson Regression Log-Likelihood Function\n\npoisson_regression_loglikelihood &lt;- function(beta, Y, X) {\n  # Linear predictor: X %*% beta gives a column vector\n  eta &lt;- X %*% beta\n  \n  # Inverse link function (log link): lambda = exp(X * beta)\n  lambda &lt;- exp(eta)\n  \n  # Log-likelihood function\n  log_lik &lt;- sum(-lambda + Y * log(lambda) - lfactorial(Y))\n  \n  return(log_lik)\n}\n\nThis function accepts: - beta: a vector of regression coefficients\n- Y: a vector of observed patent counts\n- X: a covariate matrix including firm-level predictors (e.g., age, region dummies, customer status)\nEstimating Poisson Regression with Covariates Using optim()\nWe now construct the design matrix ( X ), find the MLE of the coefficient vector ( ), and calculate standard errors using the inverse of the Hessian matrix.\nStep 1: Construct the Covariate Matrix\n\n# Create region dummies (drop one to avoid multicollinearity)\ndf &lt;- df %&gt;%\n  mutate(region = factor(region)) %&gt;%\n  mutate(age_sq = age^2)\n\nX &lt;- model.matrix(~ age + age_sq + region + iscustomer, data = df)\n\n# Outcome variable\nY &lt;- df$patents\n\n\nStep 2: Define the Negative Log-Likelihood for optim()\n\nneg_loglik_reg &lt;- function(beta, Y, X) {\n  eta &lt;- X %*% beta\n  lambda &lt;- exp(eta)\n  -sum(-lambda + Y * log(lambda) - lfactorial(Y))  # Negative log-likelihood\n}\n\n\nStep 3: Estimate MLE and Compute Hessian\n\n# Initial guess: zero vector\ninit_beta &lt;- rep(0, ncol(X))\n\n# Optimize\nfit &lt;- optim(par = init_beta,\n             fn = neg_loglik_reg,\n             Y = Y, X = X,\n             method = \"BFGS\", hessian = TRUE)\n\n# Extract estimates and variance-covariance matrix\nbeta_hat &lt;- fit$par\nhessian_mat &lt;- fit$hessian\nvcov_mat &lt;- solve(hessian_mat)  # Invert Hessian to get variance-covariance\nse_hat &lt;- sqrt(diag(vcov_mat))  # Standard errors\n\n\nStep 4: Present Results in a Table\n\n\n\nPoisson Regression Coefficients and Standard Errors\n\n\nTerm\nEstimate\nStd_Error\n\n\n\n\n(Intercept)\n-0.1257\n0.1122\n\n\nage\n0.1158\n0.0064\n\n\nage_sq\n-0.0022\n0.0001\n\n\nregionNortheast\n-0.0246\n0.0434\n\n\nregionNorthwest\n-0.0348\n0.0529\n\n\nregionSouth\n-0.0054\n0.0524\n\n\nregionSouthwest\n-0.0378\n0.0472\n\n\niscustomer\n0.0607\n0.0321\n\n\n\n\n\n\nThe table shows the estimated effect of each covariate on the log of the expected number of patents. The standard errors are derived from the inverse Hessian, assuming the log-likelihood is approximately quadratic near the maximum.\n\nChecking Results with glm()\nTo validate our MLE results, we fit the same Poisson regression model using R’s glm() function with the family = poisson option.\n\n# Fit Poisson regression using glm()\nglm_fit &lt;- glm(patents ~ age + I(age^2) + region + iscustomer,\n               data = df, family = poisson())\n\n# Summary of model\nsummary(glm_fit)\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = poisson(), data = df)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.508920   0.183179  -2.778  0.00546 ** \nage              0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)        -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast  0.029170   0.043625   0.669  0.50372    \nregionNorthwest -0.017574   0.053781  -0.327  0.74383    \nregionSouth      0.056561   0.052662   1.074  0.28281    \nregionSouthwest  0.050576   0.047198   1.072  0.28391    \niscustomer       0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nThe coefficient and standard error estimates obtained from glm() match closely with our custom implementation using optim(). This validates that our likelihood function and MLE approach are working as expected.\n\n\n\n\n\n\n\nInterpretation\n\n\n\nThe Poisson regression model estimates how various firm characteristics influence the expected number of patents awarded over the past 5 years. Key findings include:\n\nAge has a positive effect on patent output: older firms tend to secure more patents. However, the negative and statistically significant coefficient on age squared suggests diminishing returns — patent productivity increases with age, but at a decreasing rate.\nRegion effects are relatively small and statistically insignificant, indicating that geographic location (after controlling for other variables) does not strongly affect patent counts.\nMost importantly, the coefficient on iscustomer is 0.2076, which is statistically significant at the 0.001 level. Interpreted on the original scale:\n\\[\n\\exp(0.2076) - 1 \\approx 23\\%\n\\]\nThis means that, holding all else constant, firms that use Blueprinty software have an estimated 23% higher expected patent count than non-users.\n\n\nThese results support the marketing team’s claim: Blueprinty customers tend to secure more patents, even after adjusting for age and region. However, this is still a correlational model, and other unmeasured factors may contribute to this difference."
  },
  {
    "objectID": "projects/project3/hw2_questions.html#airbnb-case-study",
    "href": "projects/project3/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\nLoad and Explore the Data\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Load the Airbnb dataset\nairbnb &lt;- read_csv(\"files/airbnb.csv\")\n\n# Glimpse structure\nglimpse(airbnb)\n\nRows: 40,628\nColumns: 14\n$ ...1                      &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1…\n$ id                        &lt;dbl&gt; 2515, 2595, 3647, 3831, 4611, 5099, 5107, 51…\n$ days                      &lt;dbl&gt; 3130, 3127, 3050, 3038, 3012, 2981, 2981, 29…\n$ last_scraped              &lt;chr&gt; \"4/2/2017\", \"4/2/2017\", \"4/2/2017\", \"4/2/201…\n$ host_since                &lt;chr&gt; \"9/6/2008\", \"9/9/2008\", \"11/25/2008\", \"12/7/…\n$ room_type                 &lt;chr&gt; \"Private room\", \"Entire home/apt\", \"Private …\n$ bathrooms                 &lt;dbl&gt; 1, 1, 1, 1, NA, 1, 1, NA, 1, 1, 1, 1, 1, NA,…\n$ bedrooms                  &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2,…\n$ price                     &lt;dbl&gt; 59, 230, 150, 89, 39, 212, 250, 60, 129, 79,…\n$ number_of_reviews         &lt;dbl&gt; 150, 20, 0, 116, 93, 60, 60, 50, 53, 329, 11…\n$ review_scores_cleanliness &lt;dbl&gt; 9, 9, NA, 9, 9, 9, 10, 8, 9, 7, 10, 9, 9, 9,…\n$ review_scores_location    &lt;dbl&gt; 9, 10, NA, 9, 8, 9, 9, 9, 10, 10, 10, 9, 10,…\n$ review_scores_value       &lt;dbl&gt; 9, 9, NA, 9, 9, 9, 10, 9, 9, 9, 10, 9, 10, 9…\n$ instant_bookable          &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FAL…\n\n# Summary of missing values\ncolSums(is.na(airbnb))\n\n                     ...1                        id                      days \n                        0                         0                         0 \n             last_scraped                host_since                 room_type \n                        0                        35                         0 \n                bathrooms                  bedrooms                     price \n                      160                        76                         0 \n        number_of_reviews review_scores_cleanliness    review_scores_location \n                        0                     10195                     10254 \n      review_scores_value          instant_bookable \n                    10256                         0 \n\n\n\nWe begin by loading the dataset and checking for missing values. This helps us identify which variables may need to be cleaned or dropped before modeling.\n\n\n\nClean the Data\n\nlibrary(tidyr)  # Needed for drop_na()\n\n# Keep only relevant variables and drop rows with missing values\nairbnb_clean &lt;- airbnb %&gt;%\n  select(number_of_reviews, room_type, bathrooms, bedrooms, price,\n         review_scores_cleanliness, review_scores_location,\n         review_scores_value, instant_bookable, days) %&gt;%\n  drop_na()\n\n\nWe focus on variables likely to affect booking frequency and remove rows with missing values. This ensures our Poisson model will run without NA-related errors.\n\n\n\nExploratory Data Analysis\n\n# Distribution of number of reviews\nggplot(airbnb_clean, aes(x = number_of_reviews)) +\n  geom_histogram(bins = 50, fill = \"steelblue\") +\n  labs(title = \"Distribution of Reviews (Bookings Proxy)\",\n       x = \"Number of Reviews\", y = \"Count of Listings\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\n\nThe histogram shows that the majority of Airbnb listings receive very few reviews, with a large spike at 0–10 reviews. The distribution is highly right-skewed, with a long tail extending toward listings that have over 100 reviews.\nThis suggests that while a small number of listings are very popular, most listings receive relatively low engagement. The count nature and skewed distribution justify using a Poisson regression model to study factors that influence booking activity (as proxied by number of reviews).\n\n\n\n\n# Reviews by room type\nggplot(airbnb_clean, aes(x = room_type, y = number_of_reviews)) +\n  geom_boxplot(fill = \"tomato\") +\n  labs(title = \"Reviews by Room Type\",\n       x = \"Room Type\", y = \"Number of Reviews\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\n\nThe boxplot shows that all room types have a wide range of reviews, with many extreme outliers.\n- Private rooms appear to have a slightly higher median number of reviews than entire homes and shared rooms.\n- Shared rooms generally receive the fewest reviews, with a lower median and tighter interquartile range.\n- Entire homes/apartments show greater variability, but their central tendency is comparable to private rooms.\nThese differences suggest that room type is a relevant predictor of booking frequency and should be included in the Poisson model.\n\n\n\n\n\nFit a Poisson Regression Model\n\n# Convert categorical variables\nairbnb_clean$instant_bookable &lt;- airbnb_clean$instant_bookable == \"t\"\n\n# Fit the model\npoisson_model &lt;- glm(number_of_reviews ~ room_type + bathrooms + bedrooms +\n                       price + review_scores_cleanliness + review_scores_location +\n                       review_scores_value + instant_bookable + days,\n                     data = airbnb_clean, family = poisson())\n\n# Summary\nsummary(poisson_model)\n\n\nCall:\nglm(formula = number_of_reviews ~ room_type + bathrooms + bedrooms + \n    price + review_scores_cleanliness + review_scores_location + \n    review_scores_value + instant_bookable + days, family = poisson(), \n    data = airbnb_clean)\n\nCoefficients: (1 not defined because of singularities)\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                3.646e+00  1.595e-02 228.572  &lt; 2e-16 ***\nroom_typePrivate room      1.213e-02  2.735e-03   4.435 9.19e-06 ***\nroom_typeShared room      -2.172e-01  8.616e-03 -25.204  &lt; 2e-16 ***\nbathrooms                 -1.105e-01  3.789e-03 -29.163  &lt; 2e-16 ***\nbedrooms                   7.562e-02  2.005e-03  37.715  &lt; 2e-16 ***\nprice                     -3.697e-05  8.554e-06  -4.322 1.55e-05 ***\nreview_scores_cleanliness  1.138e-01  1.489e-03  76.419  &lt; 2e-16 ***\nreview_scores_location    -8.086e-02  1.600e-03 -50.527  &lt; 2e-16 ***\nreview_scores_value       -9.708e-02  1.795e-03 -54.091  &lt; 2e-16 ***\ninstant_bookableTRUE              NA         NA      NA       NA    \ndays                       4.962e-05  4.029e-07 123.163  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 961626  on 30159  degrees of freedom\nResidual deviance: 940403  on 30150  degrees of freedom\nAIC: 1061889\n\nNumber of Fisher Scoring iterations: 9\n\n\n\n\nVisualizing Predicted Reviews by Room Type\n\n\n\n\n\n\n\n\n\n\n\nInterpreting the Exponentiated Coefficients\n\n# View exponentiated coefficients\nexp(coef(poisson_model))\n\n              (Intercept)     room_typePrivate room      room_typeShared room \n               38.3132645                 1.0122050                 0.8047943 \n                bathrooms                  bedrooms                     price \n                0.8953793                 1.0785548                 0.9999630 \nreview_scores_cleanliness    review_scores_location       review_scores_value \n                1.1205166                 0.9223183                 0.9074845 \n     instant_bookableTRUE                      days \n                       NA                 1.0000496 \n\n\n\n\n\n\n\n\nInterpretation\n\n\n\nThis model estimates how different features of an Airbnb listing affect the expected number of reviews, which we use as a proxy for how often the listing is booked.\n\nKey Findings:\n\nRoom Type:\n\nPrivate rooms receive about 1.2% more reviews than entire homes.\n\nShared rooms receive about 19.5% fewer reviews, suggesting they are less popular.\n\nBathrooms:\nEach additional bathroom is associated with about 10.5% fewer reviews, possibly because larger properties serve a more niche market.\nBedrooms:\nListings with more bedrooms receive about 7.9% more reviews per extra bedroom, likely due to their suitability for larger groups.\nPrice:\nHigher prices slightly reduce expected bookings, though the effect is very small per dollar.\nCleanliness Score:\nA 1-point increase in cleanliness rating leads to a 12% increase in expected reviews — this is one of the strongest effects, highlighting the importance of cleanliness.\nDays Listed:\nListings that have been active longer receive more reviews, which is expected since they have more exposure.\nInstant Bookable:\nThis feature was excluded due to multicollinearity, meaning it was too similar to other variables to be separately estimated.\n\n\n\n\nSummary\n\nCleanliness, bedroom count, and room type are the most important factors for getting more bookings.\nShared rooms and expensive or oversized listings tend to get booked less often.\nStaying on the platform longer helps accumulate reviews, but maintaining high standards (especially cleanliness) is more impactful.\nThese are associations, not guarantees — but the model gives us a strong idea of what drives listing popularity on Airbnb."
  },
  {
    "objectID": "projects/project3/hw2_questions.html#estimation-of-simple-poisson-model-1",
    "href": "projects/project3/hw2_questions.html#estimation-of-simple-poisson-model-1",
    "title": "Poisson Regression Examples",
    "section": "Estimation of Simple Poisson Model",
    "text": "Estimation of Simple Poisson Model\nSince our outcome variable of interest (number of patents) is a count that can only take non-negative integers, we model it using a Poisson distribution. Specifically, we assume:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda)\n\\]\nThe probability mass function for a Poisson-distributed variable is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nWe now write the likelihood function for a sample of ( n ) independent observations ( Y_1, Y_2, , Y_n ):\n\\[\nL(\\lambda \\mid Y_1, \\dots, Y_n) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nThis simplifies to:\n\\[\nL(\\lambda) = \\frac{e^{-n\\lambda} \\lambda^{\\sum Y_i}}{\\prod_{i=1}^{n} Y_i!}\n\\]\nTaking the natural logarithm gives the log-likelihood function:\n\\[\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda - \\sum_{i=1}^{n} \\log Y_i!\n\\]\nThis function will form the basis for estimating ( ) via Maximum Likelihood Estimation (MLE).\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\nEstimation of Poisson Regression Model\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "projects/project3/hw2_questions.html#deriving-the-mle-for-λ-in-the-poisson-model",
    "href": "projects/project3/hw2_questions.html#deriving-the-mle-for-λ-in-the-poisson-model",
    "title": "Poisson Regression Examples",
    "section": "Deriving the MLE for λ in the Poisson Model",
    "text": "Deriving the MLE for λ in the Poisson Model\nWe begin with the log-likelihood function for a Poisson-distributed variable ( Y_1, , Y_n () ):\n\\[\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda - \\sum_{i=1}^{n} \\log Y_i!\n\\]\nSince the last term does not depend on ( ), we focus on the first two terms when maximizing:\n\\[\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum Y_i \\right) \\log \\lambda\n\\]\nTaking the derivative with respect to ( ):\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = -n + \\frac{\\sum Y_i}{\\lambda}\n\\]\nSet the derivative equal to zero to find the maximum:\n\\[\n-n + \\frac{\\sum Y_i}{\\lambda} = 0\n\\]\nSolving for ( ):\n\\[\n\\lambda_{\\text{MLE}} = \\frac{1}{n} \\sum Y_i = \\bar{Y}\n\\]\nThis result aligns with intuition: the mean of a Poisson distribution is ( ), so the sample mean is the natural estimator.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python. #### Finding the MLE Using optim()\n\n# Negative log-likelihood (since optim minimizes)\nneg_loglik &lt;- function(lambda, Y) {\n  return(-poisson_loglikelihood(lambda, Y))\n}\n\n# Use optim() to find lambda that minimizes the negative log-likelihood\nmle_result &lt;- optim(par = 1, fn = neg_loglik, Y = df$patents, method = \"Brent\", lower = 0.01, upper = 10)\n\n# Print MLE estimate\nlambda_mle &lt;- mle_result$par\nlambda_mle\n\n[1] 3.684667\n\n\n\nEstimation of Poisson Regression Model\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "projects/project3/hw2_questions.html#log-likelihood-plot-with-mle",
    "href": "projects/project3/hw2_questions.html#log-likelihood-plot-with-mle",
    "title": "Poisson Regression Examples",
    "section": "Log-Likelihood Plot with MLE",
    "text": "Log-Likelihood Plot with MLE\n\n\n\n\n\n\n\n\n\n\nEstimation of Poisson Regression Model\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "projects/project3/hw2_questions.html#checking-results-with-glm",
    "href": "projects/project3/hw2_questions.html#checking-results-with-glm",
    "title": "Poisson Regression Examples",
    "section": "Checking Results with glm()",
    "text": "Checking Results with glm()\nTo validate our MLE results, we fit the same Poisson regression model using R’s glm() function with the family = poisson option.\n\n# Fit Poisson regression using glm()\nglm_fit &lt;- glm(patents ~ age + I(age^2) + region + iscustomer,\n               data = df, family = poisson())\n\n# Summary of model\nsummary(glm_fit)\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = poisson(), data = df)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.508920   0.183179  -2.778  0.00546 ** \nage              0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)        -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast  0.029170   0.043625   0.669  0.50372    \nregionNorthwest -0.017574   0.053781  -0.327  0.74383    \nregionSouth      0.056561   0.052662   1.074  0.28281    \nregionSouthwest  0.050576   0.047198   1.072  0.28391    \niscustomer       0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "projects/project3/hw2_questions.html#estimating-the-effect-of-blueprinty-software",
    "href": "projects/project3/hw2_questions.html#estimating-the-effect-of-blueprinty-software",
    "title": "Poisson Regression Examples",
    "section": "Estimating the Effect of Blueprinty Software",
    "text": "Estimating the Effect of Blueprinty Software\nSince Poisson regression coefficients are on the log scale and not directly interpretable, we estimate the average marginal effect of using Blueprinty software by creating two hypothetical scenarios:\n\nX_0: All firms are non-customers (iscustomer = 0)\nX_1: All firms are customers (iscustomer = 1)\n\nWe then predict patent counts in both cases using the fitted model and compare the difference.\n\n# Step 1: Create X_0 and X_1\nX_0 &lt;- X\nX_1 &lt;- X\nX_0[, \"iscustomer\"] &lt;- 0\nX_1[, \"iscustomer\"] &lt;- 1\n\n# Step 2: Predicted patent counts\neta_0 &lt;- X_0 %*% beta_hat\neta_1 &lt;- X_1 %*% beta_hat\ny_pred_0 &lt;- exp(eta_0)\ny_pred_1 &lt;- exp(eta_1)\n\n# Step 3: Average difference\neffect &lt;- mean(y_pred_1 - y_pred_0)\neffect\n\n[1] 0.2178843\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\n\nOn average, firms predicted to be Blueprinty customers have 0.22 more patents than if they were not customers, holding age and region constant.\nThis provides an interpretable estimate of Blueprinty’s marginal effect and supports the claim that Blueprinty usage is associated with increased patent success."
  },
  {
    "objectID": "projects/project4/hw3_questions.html",
    "href": "projects/project4/hw3_questions.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "projects/project4/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "projects/project4/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "projects/project4/hw3_questions.html#simulate-conjoint-data",
    "href": "projects/project4/hw3_questions.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n# set seed for reproducibility\nset.seed(123)\n\n# define attributes\nbrand &lt;- c(\"N\", \"P\", \"H\") # Netflix, Prime, Hulu\nad &lt;- c(\"Yes\", \"No\")\nprice &lt;- seq(8, 32, by=4)\n\n# generate all possible profiles\nprofiles &lt;- expand.grid(\n    brand = brand,\n    ad = ad,\n    price = price\n)\nm &lt;- nrow(profiles)\n\n# assign part-worth utilities (true parameters)\nb_util &lt;- c(N = 1.0, P = 0.5, H = 0)\na_util &lt;- c(Yes = -0.8, No = 0.0)\np_util &lt;- function(p) -0.1 * p\n\n# number of respondents, choice tasks, and alternatives per task\nn_peeps &lt;- 100\nn_tasks &lt;- 10\nn_alts &lt;- 3\n\n# function to simulate one respondent’s data\nsim_one &lt;- function(id) {\n  \n    datlist &lt;- list()\n    \n    # loop over choice tasks\n    for (t in 1:n_tasks) {\n        \n        # randomly sample 3 alts (better practice would be to use a design)\n        dat &lt;- cbind(resp=id, task=t, profiles[sample(m, size=n_alts), ])\n        \n        # compute deterministic portion of utility\n        dat$v &lt;- b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price) |&gt; round(10)\n        \n        # add Gumbel noise (Type I extreme value)\n        dat$e &lt;- -log(-log(runif(n_alts)))\n        dat$u &lt;- dat$v + dat$e\n        \n        # identify chosen alternative\n        dat$choice &lt;- as.integer(dat$u == max(dat$u))\n        \n        # store task\n        datlist[[t]] &lt;- dat\n    }\n    \n    # combine all tasks for one respondent\n    do.call(rbind, datlist)\n}\n\n# simulate data for all respondents\nconjoint_data &lt;- do.call(rbind, lapply(1:n_peeps, sim_one))\n\n# remove values unobservable to the researcher\nconjoint_data &lt;- conjoint_data[ , c(\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\")]\n\n# clean up\nrm(list=setdiff(ls(), \"conjoint_data\"))"
  },
  {
    "objectID": "projects/project4/hw3_questions.html#preparing-the-data-for-estimation",
    "href": "projects/project4/hw3_questions.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\nTo prepare the dataset for estimation, we convert categorical variables (brand and ad) into dummy variables. We use Netflix (N) as the base for brand, and “No Ads” as the base for ad. This structure will allow us to easily compute utilities and evaluate likelihood functions.\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Load the simulated conjoint data\nconjoint &lt;- read_csv(\"files/conjoint_data.csv\")\n\n\n# Recode factors\nconjoint &lt;- conjoint %&gt;%\n  mutate(\n    brand = factor(brand),\n    ad = factor(ad),\n    price = as.numeric(price),\n    brand_P = ifelse(brand == \"P\", 1, 0),\n    brand_H = ifelse(brand == \"H\", 1, 0),\n    ad_yes = ifelse(ad == \"Yes\", 1, 0)\n  )\n\n# Check structure\nhead(conjoint)\n\n# A tibble: 6 × 9\n   resp  task choice brand ad    price brand_P brand_H ad_yes\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1     1     1      1 N     Yes      28       0       0      1\n2     1     1      0 H     Yes      16       0       1      1\n3     1     1      0 P     Yes      16       1       0      1\n4     1     2      0 N     Yes      32       0       0      1\n5     1     2      1 P     Yes      16       1       0      1\n6     1     2      0 N     Yes      24       0       0      1"
  },
  {
    "objectID": "projects/project4/hw3_questions.html#estimation-via-maximum-likelihood",
    "href": "projects/project4/hw3_questions.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\nThe log-likelihood function for a set of parameters \\(\\beta\\) is defined as:\n\\[\n\\ell(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J y_{ij} \\log \\left( \\frac{e^{X_{ij} \\beta}}{\\sum_k e^{X_{ik} \\beta}} \\right)\n\\]\nWe implement this using R’s optim() function.\n\n# Define the log-likelihood function for MNL\nmnl_log_likelihood &lt;- function(beta, data) {\n  # Extract variables\n  X &lt;- as.matrix(data[, c(\"brand_P\", \"brand_H\", \"ad_yes\", \"price\")])\n  y &lt;- data$choice\n  n &lt;- nrow(X)\n  \n  # Compute utility\n  utilities &lt;- X %*% beta\n  \n  # Create group IDs for each choice set\n  group_ids &lt;- paste(data$resp, data$task, sep = \"_\")\n  \n  # Compute denominator of MNL (sum of exp utilities within choice sets)\n  denom &lt;- ave(exp(utilities), group_ids, FUN = function(x) sum(x))\n  \n  # Compute individual probabilities\n  probs &lt;- exp(utilities) / denom\n  \n  # Compute log-likelihood\n  log_likelihood &lt;- sum(y * log(probs))\n  \n  return(-log_likelihood)  # negative for minimization\n}\n\n# Initial guesses\ninit_beta &lt;- rep(0, 4)  # one for each variable\n\n# Maximize log-likelihood\nmnl_fit &lt;- optim(\n  par = init_beta,\n  fn = mnl_log_likelihood,\n  data = conjoint,\n  method = \"BFGS\",\n  hessian = TRUE\n)\n\n# Extract coefficients\nmnl_coefs &lt;- mnl_fit$par\nnames(mnl_coefs) &lt;- c(\"brand_P\", \"brand_H\", \"ad_yes\", \"price\")\nmnl_coefs\n\n    brand_P     brand_H      ad_yes       price \n-0.43957167 -0.94119432 -0.73200405 -0.09948116 \n\n\n\nInterpretation of MLE Coefficient Estimates\n\n\n\n\n\n\n\n\nVariable\nEstimate\nInterpretation\n\n\n\n\nbrand_P\n–0.440\nHolding all else constant, Amazon Prime is less preferred than Netflix (the base brand), reducing utility by 0.44 units.\n\n\nbrand_H\n–0.941\nHulu is the least preferred brand — its presence reduces utility by 0.94 units relative to Netflix.\n\n\nad_yes\n–0.732\nIncluding advertisements significantly lowers utility (by 0.73 units), consistent with consumers disliking ads.\n\n\nprice\n–0.099\nEach additional dollar in monthly price reduces utility by about 0.10 units — a strong and expected negative price sensitivity.\n\n\n\n\n\n\n\n\n\nSummary of Results\n\n\n\n\nBrand Preferences: Consumers prefer Netflix the most, followed by Amazon Prime, with Hulu being the least preferred. This pattern aligns with the true simulated part-worths: Netflix = 1.0, Prime = 0.5, and Hulu = 0.\nAd Sensitivity: There is a strong negative coefficient for ad_yes, indicating that consumers clearly dislike ad-supported streaming plans.\nPrice Sensitivity: The model captures a realistic and expected negative relationship between price and utility. The estimated coefficient (–0.099) closely matches the true simulated value of –0.1.\n\n\n\nAfter estimating the model via optim(), we can extract the Hessian matrix to approximate the variance-covariance matrix of the MLEs. The diagonal of the inverse Hessian gives us the variances, and we compute standard errors and 95% confidence intervals accordingly.\n\n# Extract estimated parameters\nestimates &lt;- mnl_fit$par\nnames(estimates) &lt;- c(\"brand_P\", \"brand_H\", \"ad_yes\", \"price\")\n\n# Compute standard errors from inverse Hessian\nvcov_matrix &lt;- solve(mnl_fit$hessian)\nstd_errors &lt;- sqrt(diag(vcov_matrix))\n\n# 95% confidence intervals\nz_value &lt;- qnorm(0.975)  # ≈ 1.96\nci_lower &lt;- estimates - z_value * std_errors\nci_upper &lt;- estimates + z_value * std_errors\n\n# Combine into a table\nresults_df &lt;- data.frame(\n  Parameter = names(estimates),\n  Estimate = round(estimates, 3),\n  StdError = round(std_errors, 3),\n  CI_Lower = round(ci_lower, 3),\n  CI_Upper = round(ci_upper, 3)\n)\n\nlibrary(knitr)\nkable(results_df, caption = \"MLE Estimates with Standard Errors and 95% Confidence Intervals\")\n\n\nMLE Estimates with Standard Errors and 95% Confidence Intervals\n\n\n\nParameter\nEstimate\nStdError\nCI_Lower\nCI_Upper\n\n\n\n\nbrand_P\nbrand_P\n-0.440\n0.104\n-0.644\n-0.236\n\n\nbrand_H\nbrand_H\n-0.941\n0.111\n-1.159\n-0.724\n\n\nad_yes\nad_yes\n-0.732\n0.088\n-0.904\n-0.560\n\n\nprice\nprice\n-0.099\n0.006\n-0.112\n-0.087\n\n\n\n\n\n\n\n\n\n\n\nInterpretation of Confidence Intervals\n\n\n\n\nAmazon Prime is significantly less preferred than Netflix. Since the CI does not include 0, this effect is statistically significant.\nHulu is much less preferred than Netflix. This large, negative, and statistically significant estimate shows strong consumer aversion to Hulu.\nIncluding ads significantly reduces utility. Consumers have a clear preference for ad-free streaming services.\nThe price coefficient is statistically significant and negative, meaning that as price increases, utility decreases—consistent with economic theory.\n\n\n\nSummary: All four parameters are statistically significant at the 95% confidence level since none of their intervals include zero. The direction and magnitude of the effects align well with both theoretical expectations and the simulated ground truth:\n\nNetflix is the preferred brand (baseline).\nConsumers dislike ads and are sensitive to higher prices.\nHulu has the largest negative utility among brands."
  },
  {
    "objectID": "projects/project4/hw3_questions.html#estimation-via-bayesian-methods",
    "href": "projects/project4/hw3_questions.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\n\nBayesian Estimation using Metropolis-Hastings\nWe now estimate the posterior distribution of the MNL model parameters using a Metropolis-Hastings MCMC sampler. We specify the following priors:\n\n\\(\\beta_\\text{brand\\_P}, \\beta_\\text{brand\\_H}, \\beta_\\text{ad\\_yes} \\sim \\mathcal{N}(0, 5)\\)\n\\(\\beta_\\text{price} \\sim \\mathcal{N}(0, 1)\\)\n\nWe take 11,000 total MCMC samples and discard the first 1,000 as burn-in. The proposal distribution is symmetric and diagonal: - \\(\\mathcal{N}(0, 0.05)\\) for the binary-variable betas - \\(\\mathcal{N}(0, 0.005)\\) for the price beta\n\n# Reuse the log-likelihood function from earlier\nlog_likelihood &lt;- function(beta, data) {\n  X &lt;- as.matrix(data[, c(\"brand_P\", \"brand_H\", \"ad_yes\", \"price\")])\n  y &lt;- data$choice\n  util &lt;- X %*% beta\n  group_ids &lt;- paste(data$resp, data$task, sep = \"_\")\n  denom &lt;- ave(exp(util), group_ids, FUN = sum)\n  prob &lt;- exp(util) / denom\n  log_lik &lt;- sum(y * log(prob))\n  return(log_lik)\n}\n\n# Define log-prior\nlog_prior &lt;- function(beta) {\n  lp &lt;- dnorm(beta[1], 0, sqrt(5), log=TRUE) + \n        dnorm(beta[2], 0, sqrt(5), log=TRUE) + \n        dnorm(beta[3], 0, sqrt(5), log=TRUE) + \n        dnorm(beta[4], 0, sqrt(1), log=TRUE)\n  return(lp)\n}\n\n# Log-posterior = log-likelihood + log-prior\nlog_posterior &lt;- function(beta, data) {\n  log_likelihood(beta, data) + log_prior(beta)\n}\n\n# Metropolis-Hastings sampler\nrun_mcmc &lt;- function(data, n_iter = 11000, proposal_sd = c(0.05, 0.05, 0.05, 0.005)) {\n  draws &lt;- matrix(NA, nrow = n_iter, ncol = 4)\n  colnames(draws) &lt;- c(\"brand_P\", \"brand_H\", \"ad_yes\", \"price\")\n  \n  beta_current &lt;- c(0, 0, 0, 0)\n  logpost_current &lt;- log_posterior(beta_current, data)\n  \n  for (i in 1:n_iter) {\n    beta_proposed &lt;- beta_current + rnorm(4, mean = 0, sd = proposal_sd)\n    logpost_proposed &lt;- log_posterior(beta_proposed, data)\n    \n    accept_prob &lt;- exp(logpost_proposed - logpost_current)\n    if (runif(1) &lt; accept_prob) {\n      beta_current &lt;- beta_proposed\n      logpost_current &lt;- logpost_proposed\n    }\n    draws[i, ] &lt;- beta_current\n  }\n  return(draws)\n}\n\n# Run MCMC\nset.seed(42)\nmcmc_draws &lt;- run_mcmc(conjoint)\n\n# Remove burn-in\nmcmc_posterior &lt;- mcmc_draws[1001:11000, ]\n\nposterior_summary &lt;- apply(mcmc_posterior, 2, function(x) {\n  c(mean = mean(x),\n    sd = sd(x),\n    `2.5%` = quantile(x, 0.025),\n    `97.5%` = quantile(x, 0.975))\n})\n\n\n\n\n\n\n\nSummary\n\n\n\n\n\nAll four parameters are statistically meaningful: their 95% credible intervals do not include 0.\nThe Bayesian posterior estimates are very similar to the MLEs — this supports that both inference methods are consistent with the data and the underlying true utility model.\nThe credible intervals are tight, reflecting high certainty in your parameter estimates given the data and priors.\n\n\n\n\n\n\nPosterior Diagnostics for brand_P\nTo assess convergence and sampling behavior, we examine the trace plot and posterior histogram for the parameter brand_P.\n\n# Trace plot for brand_P\nplot(mcmc_posterior[, \"brand_P\"], type = \"l\",\n     main = \"Trace Plot: brand_P\",\n     xlab = \"Iteration\", ylab = \"Value\")\n\n\n\n\n\n\n\n\n\n# Histogram / posterior density for brand_P\nhist(mcmc_posterior[, \"brand_P\"], breaks = 40, probability = TRUE,\n     main = \"Posterior Distribution: brand_P\",\n     xlab = \"Value\", col = \"skyblue\", border = \"white\")\nlines(density(mcmc_posterior[, \"brand_P\"]), col = \"darkblue\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\n\nTo assess the convergence and distribution of the Bayesian estimate for brand_P, we examine both the trace plot and posterior histogram.\nTrace Plot\nThe trace plot of brand_P over 10,000 post–burn-in iterations shows stable and well-mixed behavior. The chain fluctuates around a consistent central region, suggesting good convergence and no signs of non-stationarity or drift. This supports the validity of the posterior estimates.\nPosterior Distribution\nThe histogram and density plot for brand_P reveal a roughly symmetric and unimodal distribution centered near –0.43. This aligns well with both the MLE and true simulated value, indicating that the posterior is well-informed by the data. The shape of the distribution also suggests reasonable uncertainty, with most mass concentrated between –0.63 and –0.23.\nTogether, these plots confirm that the MCMC sampler produced reliable draws from the posterior distribution for brand_P.\n\n\n\n\n\n\n\n\n\nPosterior Mens\n\n\n\n\n\n\n# MLE estimates\nmle_estimates &lt;- c(brand_P = -0.440, brand_H = -0.941, ad_yes = -0.732, price = -0.099)\n\n# Compute Bayesian summaries directly\nposterior_means &lt;- colMeans(mcmc_posterior)\nposterior_sds &lt;- apply(mcmc_posterior, 2, sd)\nposterior_cis &lt;- apply(mcmc_posterior, 2, quantile, probs = c(0.025, 0.975))\n\n# Build comparison data frame\nposterior_df &lt;- data.frame(\n  Parameter = names(mle_estimates),\n  MLE = round(mle_estimates, 3),\n  Posterior_Mean = round(posterior_means, 3),\n  Posterior_SD = round(posterior_sds, 3),\n  CI_Lower = round(posterior_cis[1, ], 3),\n  CI_Upper = round(posterior_cis[2, ], 3),\n  row.names = NULL\n)\n\n\n\n\n\n# Display table\nlibrary(knitr)\nkable(posterior_df, caption = \"Comparison of MLE and Bayesian Posterior Estimates\")\n\n\nComparison of MLE and Bayesian Posterior Estimates\n\n\nParameter\nMLE\nPosterior_Mean\nPosterior_SD\nCI_Lower\nCI_Upper\n\n\n\n\nbrand_P\n-0.440\n-0.429\n0.103\n-0.628\n-0.232\n\n\nbrand_H\n-0.941\n-0.941\n0.114\n-1.165\n-0.721\n\n\nad_yes\n-0.732\n-0.732\n0.090\n-0.911\n-0.558\n\n\nprice\n-0.099\n-0.100\n0.006\n-0.112\n-0.087\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\n\n\nThe posterior means are nearly identical to the MLEs, confirming that the data are highly informative and that the choice of priors had little influence.\nThe posterior standard deviations provide a similar sense of uncertainty as the standard errors from MLE.\nThe 95% credible intervals are also close to the confidence intervals, but offer a Bayesian interpretation — we can say there is a 95% probability the parameter lies within the interval, given the data and prior.\nThis close agreement between Bayesian and frequentist results suggests that the model is well-specified, and both estimation approaches yield consistent and reliable parameter estimates."
  },
  {
    "objectID": "projects/project4/hw3_questions.html#discussion",
    "href": "projects/project4/hw3_questions.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\nIf we had not simulated the data ourselves, we could still draw meaningful insights from the estimated parameters based solely on model output. All parameter estimates — from both the MLE and Bayesian approaches — are directionally consistent, statistically significant, and aligned with consumer intuition.\n\nThe fact that \\(\\beta_\\text{Prime} &lt; \\beta_\\text{Netflix}\\) suggests that, all else equal, consumers prefer Netflix over Amazon Prime. This preference is encoded in the utility function, and the magnitude of the difference reflects how strongly Netflix dominates Prime in consumers’ choices.\nSimilarly, \\(\\beta_\\text{Hulu}\\) being even more negative confirms Hulu is the least preferred brand among the three. Without simulation knowledge, we can still conclude that Hulu is less attractive based on choice behavior alone.\nA negative \\(\\beta_\\text{price}\\) indicates that, holding other attributes constant, higher prices reduce the probability of an alternative being chosen. This makes intuitive economic sense and is consistent with downward-sloping demand — consumers prefer cheaper plans, all else equal.\n\nTaken together, these estimates reflect a reasonable and interpretable preference structure that validates the utility-based choice model, even without knowing the “true” data-generating process.\n\nExtending to a Hierarchical (Random-Parameter) Logit Model\nTo simulate data from — and estimate parameters of — a multi-level (hierarchical or random-parameter) logit model, we would need to introduce heterogeneity in preferences across individuals. In other words, rather than assuming a single, fixed set of utility weights (betas) shared by all respondents, we would allow each individual to have their own \\(\\boldsymbol{\\beta}_i\\), drawn from a population-level distribution.\n\n\nChanges Required for Simulation:\n\nIndividual-level variation: Instead of using a fixed \\(\\boldsymbol{\\beta}\\) for all, we would simulate each respondent’s \\(\\boldsymbol{\\beta}_i\\) from a multivariate normal distribution: \\[\n\\boldsymbol{\\beta}_i \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\n\\] where \\(\\boldsymbol{\\mu}\\) is the population mean vector and \\(\\boldsymbol{\\Sigma}\\) is the covariance matrix capturing between-individual variability and correlation between preferences.\nData generation: When simulating choices, each respondent’s utilities would be computed using their own \\(\\boldsymbol{\\beta}_i\\), leading to more realistic (and noisier) variation in choices across people.\n\n\n\nChanges Required for Estimation:\n\nHierarchical modeling: We would need to estimate both levels:\n\nThe distribution of \\(\\boldsymbol{\\beta}_i\\) across individuals (i.e., \\(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}\\)).\nThe individual-level \\(\\boldsymbol{\\beta}_i\\) values conditional on observed choices.\n\nBayesian MCMC or Mixed Logit tools: Estimation is more complex and typically handled using:\n\nBayesian hierarchical modeling with Gibbs sampling or Hamiltonian Monte Carlo (e.g., via Stan, PyMC, or rstanarm), or\nFrequentist methods using simulation-based maximum likelihood (e.g., mixed logit with mlogit or apollo packages in R).\n\n\nThis hierarchical structure allows us to capture taste heterogeneity — a key feature in real-world conjoint analysis — and leads to more personalized predictions, better model fit, and richer insights into market segmentation."
  },
  {
    "objectID": "projects/project4/hw3_questions.html#discussion-1",
    "href": "projects/project4/hw3_questions.html#discussion-1",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\nIf we had not simulated the data ourselves, we could still draw meaningful insights from the estimated parameters based solely on model output. All parameter estimates — from both the MLE and Bayesian approaches — are directionally consistent, statistically significant, and aligned with consumer intuition.\n\nThe fact that \\(\\beta_\\text{Prime} &lt; \\beta_\\text{Netflix}\\) suggests that, all else equal, consumers prefer Netflix over Amazon Prime. This preference is encoded in the utility function, and the magnitude of the difference reflects how strongly Netflix dominates Prime in consumers’ choices.\nSimilarly, \\(\\beta_\\text{Hulu}\\) being even more negative confirms Hulu is the least preferred brand among the three. Without simulation knowledge, we can still conclude that Hulu is less attractive based on choice behavior alone.\nA negative \\(\\beta_\\text{price}\\) indicates that, holding other attributes constant, higher prices reduce the probability of an alternative being chosen. This makes intuitive economic sense and is consistent with downward-sloping demand — consumers prefer cheaper plans, all else equal.\n\nTaken together, these estimates reflect a reasonable and interpretable preference structure that validates the utility-based choice model, even without knowing the “true” data-generating process.\ntodo: At a high level, discuss what change you would need to make in order to simulate data from — and estimate the parameters of — a multi-level (aka random-parameter or hierarchical) model. This is the model we use to analyze “real world” conjoint data.\n\nExtending to a Hierarchical (Random-Parameter) Logit Model\nTo simulate data from — and estimate parameters of — a multi-level (hierarchical or random-parameter) logit model, we would need to introduce heterogeneity in preferences across individuals. In other words, rather than assuming a single, fixed set of utility weights (betas) shared by all respondents, we would allow each individual to have their own \\(\\boldsymbol{\\beta}_i\\), drawn from a population-level distribution.\n\n\nChanges Required for Simulation:\n\nIndividual-level variation: Instead of using a fixed \\(\\boldsymbol{\\beta}\\) for all, we would simulate each respondent’s \\(\\boldsymbol{\\beta}_i\\) from a multivariate normal distribution: \\[\n\\boldsymbol{\\beta}_i \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\n\\] where \\(\\boldsymbol{\\mu}\\) is the population mean vector and \\(\\boldsymbol{\\Sigma}\\) is the covariance matrix capturing between-individual variability and correlation between preferences.\nData generation: When simulating choices, each respondent’s utilities would be computed using their own \\(\\boldsymbol{\\beta}_i\\), leading to more realistic (and noisier) variation in choices across people.\n\n\n\nChanges Required for Estimation:\n\nHierarchical modeling: We would need to estimate both levels:\n\nThe distribution of \\(\\boldsymbol{\\beta}_i\\) across individuals (i.e., \\(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}\\)).\nThe individual-level \\(\\boldsymbol{\\beta}_i\\) values conditional on observed choices.\n\nBayesian MCMC or Mixed Logit tools: Estimation is more complex and typically handled using:\n\nBayesian hierarchical modeling with Gibbs sampling or Hamiltonian Monte Carlo (e.g., via Stan, PyMC, or rstanarm), or\nFrequentist methods using simulation-based maximum likelihood (e.g., mixed logit with mlogit or apollo packages in R).\n\n\nThis hierarchical structure allows us to capture taste heterogeneity — a key feature in real-world conjoint analysis — and leads to more personalized predictions, better model fit, and richer insights into market segmentation."
  }
]